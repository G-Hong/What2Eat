# -*- coding: utf-8 -*-
"""Llama활용 박용우봇.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KPobMQUozUt_NMRP-aezObpkynoGHYYD
"""

!pip install langchain langchain-community sentence-transformers chromadb transformers torch

!pip install langchain-huggingface

import os
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import torch
from IPython.display import clear_output

# 데이터 디렉토리 생성 및 샘플 데이터 준비
if not os.path.exists("data"):
    os.makedirs("data")

sample_text = """
인공지능(AI)은 인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현한 컴퓨터 프로그램 또는 이를 포함한 컴퓨터 시스템입니다.
머신러닝은 AI의 한 분야로, 데이터로부터 학습하여 패턴을 찾아내는 방법을 연구합니다.
딥러닝은 머신러닝의 한 종류로, 인공신경망을 기반으로 데이터로부터 스스로 특징을 학습합니다.
"""

with open("data/sample.txt", "w", encoding="utf-8") as f:
    f.write(sample_text)

# GPU 사용 가능 여부 확인
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 모델과 토크나이저 초기화
model_id = "MBZUAI/LaMini-Flan-T5-248M"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

# Text generation pipeline 생성
pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.95,
    repetition_penalty=1.15
)

# Langchain용 LLM 생성
llm = HuggingFacePipeline(pipeline=pipe)

# 한국어 임베딩 모델 초기화
embeddings = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",
    model_kwargs={'device': device}
)

# 텍스트 분할기 설정
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
    separators=["\n\n", "\n", ".", "!", "?", ",", " "]
)

# 데이터 로드 및 처리
loader = DirectoryLoader("data", glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()
texts = text_splitter.split_documents(documents)

# 벡터 DB 생성
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory="./vectorstore"
)

# 메모리 설정
memory = ConversationBufferMemory(
    memory_key="chat_history",
    output_key="answer",
    return_messages=True
)

# QA 체인 생성
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 1}),
    memory=memory,
    return_source_documents=True,
    verbose=True
)

# 대화형 인터페이스
def chat():
    print("AI 챗봇이 시작되었습니다. 종료하려면 'quit' 또는 'exit'를 입력하세요.")
    print("-" * 50)

    while True:
        user_input = input("\n사용자: ")

        if user_input.lower() in ['quit', 'exit']:
            print("\nAI 챗봇을 종료합니다.")
            break

        try:
            # 응답 생성
            response = qa_chain.invoke({"question": user_input})

            print("\nAI: ", response['answer'])
            print("\n참고 문서:")
            for i, doc in enumerate(response['source_documents'], 1):
                print(f"\n문서 {i}: {doc.page_content}")
            print("-" * 50)

        except Exception as e:
            print(f"\n오류가 발생했습니다: {str(e)}")

# 챗봇 실행
chat()

import os
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.prompts import PromptTemplate
import torch
from IPython.display import clear_output

# 데이터 디렉토리 생성 및 샘플 데이터 준비
if not os.path.exists("data"):
    os.makedirs("data")

sample_text = """
인공지능(AI)은 인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현한 컴퓨터 프로그램 또는 이를 포함한 컴퓨터 시스템입니다.
머신러닝은 AI의 한 분야로, 데이터로부터 학습하여 패턴을 찾아내는 방법을 연구합니다.
딥러닝은 머신러닝의 한 종류로, 인공신경망을 기반으로 데이터로부터 스스로 특징을 학습합니다.
"""

with open("data/sample.txt", "w", encoding="utf-8") as f:
    f.write(sample_text)

# GPU 사용 가능 여부 확인
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 모델과 토크나이저 초기화
model_id = "MBZUAI/LaMini-Flan-T5-248M"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

# Text generation pipeline 생성
pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.95,
    repetition_penalty=1.15,
    return_full_text=True,
    task_specific_params={
        "text2text-generation": {
            "prefix": "Answer in Korean: "
        }
    }
)

# Langchain용 LLM 생성
llm = HuggingFacePipeline(pipeline=pipe)

# 한국어 임베딩 모델 초기화
embeddings = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",
    model_kwargs={'device': device}
)

# 텍스트 분할기 설정
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
    separators=["\n\n", "\n", ".", "!", "?", ",", " "]
)

# 데이터 로드 및 처리
loader = DirectoryLoader("data", glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()
texts = text_splitter.split_documents(documents)

# 벡터 DB 생성
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory="./vectorstore"
)

# 메모리 설정
memory = ConversationBufferMemory(
    memory_key="chat_history",
    output_key="answer",
    return_messages=True
)

# 프롬프트 템플릿 설정
template = """아래 내용에 대해 한국어로 답변을 생성해주세요.

질문: {question}

참고 정보:
{context}

한국어로 답변해주세요:"""

PROMPT = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# QA 체인 생성
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 1}),
    memory=memory,
    return_source_documents=True,
    verbose=True,
    combine_docs_chain_kwargs={'prompt': PROMPT}
)

# 대화형 인터페이스
def chat():
    print("AI 챗봇이 시작되었습니다. 종료하려면 'quit' 또는 'exit'를 입력하세요.")
    print("-" * 50)

    while True:
        user_input = input("\n사용자: ")

        if user_input.lower() in ['quit', 'exit']:
            print("\nAI 챗봇을 종료합니다.")
            break

        try:
            # 기본 응답 처리
            if user_input.strip().lower() == "안녕":
                print("\nAI: 안녕하세요! 무엇을 도와드릴까요?")
                continue

            # 일반 질문 처리
            response = qa_chain.invoke({
                "question": f"다음 질문에 한국어로 자세히 답변해주세요: {user_input}"
            })

            # 응답이 비어있는 경우 처리
            answer = response['answer'].strip()
            if not answer:
                answer = "죄송합니다. 다시 한 번 질문해 주시겠어요?"

            print("\nAI: ", answer)
            print("\n참고 문서:")
            for i, doc in enumerate(response['source_documents'], 1):
                print(f"\n문서 {i}: {doc.page_content}")
            print("-" * 50)

        except Exception as e:
            print(f"\n오류가 발생했습니다: {str(e)}")

# 챗봇 실행
chat()

import os
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.prompts import PromptTemplate
import torch
from IPython.display import clear_output

# 데이터 디렉토리 생성
if not os.path.exists("data"):
    os.makedirs("data")

# info.txt의 내용을 참고 정보로 사용
with open("info.txt", "r", encoding="utf-8") as source_file:
    paste_content = source_file.read()

# data 폴더에 복사
with open("data/info.txt", "w", encoding="utf-8") as target_file:
    target_file.write(paste_content)

# GPU 사용 가능 여부 확인
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 모델과 토크나이저 초기화
model_id = "MBZUAI/LaMini-Flan-T5-248M"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

# Text generation pipeline 생성
pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.95,
    repetition_penalty=1.15,
    return_full_text=True,
    task_specific_params={
        "text2text-generation": {
            "prefix": "Answer in Korean: "
        }
    }
)

# Langchain용 LLM 생성
llm = HuggingFacePipeline(pipeline=pipe)

# 한국어 임베딩 모델 초기화
embeddings = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",
    model_kwargs={'device': device}
)

# 텍스트 분할기 설정
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
    separators=["\n\n", "\n", ".", "!", "?", ",", " "]
)

# 데이터 로드 및 처리
loader = DirectoryLoader("data", glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()
texts = text_splitter.split_documents(documents)

# 벡터 DB 생성
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory="./vectorstore"
)

# 메모리 설정
memory = ConversationBufferMemory(
    memory_key="chat_history",
    output_key="answer",
    return_messages=True
)

# 프롬프트 템플릿 설정
template = """아래 내용에 대해 한국어로 답변을 생성해주세요.

질문: {question}

참고 정보:
{context}

한국어로 답변해주세요:"""

PROMPT = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# QA 체인 생성
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 1}),
    memory=memory,
    return_source_documents=True,
    verbose=True,
    combine_docs_chain_kwargs={'prompt': PROMPT}
)

# 대화형 인터페이스
def chat():
    print("AI 챗봇이 시작되었습니다. 종료하려면 'quit' 또는 'exit'를 입력하세요.")
    print("-" * 50)

    while True:
        user_input = input("\n사용자: ")

        if user_input.lower() in ['quit', 'exit']:
            print("\nAI 챗봇을 종료합니다.")
            break

        try:
            # 기본 응답 처리
            if user_input.strip().lower() == "안녕":
                print("\nAI: 안녕하세요! 무엇을 도와드릴까요?")
                continue

            # 일반 질문 처리
            response = qa_chain.invoke({
                "question": f"다음 질문에 한국어로 자세히 답변해주세요: {user_input}"
            })

            # 응답이 비어있는 경우 처리
            answer = response['answer'].strip()
            if not answer:
                answer = "죄송합니다. 다시 한 번 질문해 주시겠어요?"

            print("\nAI: ", answer)
            print("\n참고 문서:")
            for i, doc in enumerate(response['source_documents'], 1):
                print(f"\n문서 {i}: {doc.page_content}")
            print("-" * 50)

        except Exception as e:
            print(f"\n오류가 발생했습니다: {str(e)}")

# 챗봇 실행
chat()

import os
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.prompts import PromptTemplate
import torch
from IPython.display import clear_output

# 데이터 디렉토리 생성 및 샘플 데이터 준비
if not os.path.exists("data"):
   os.makedirs("data")

# 다이어트 정보 준비
diet_info = """
다이어트 관련 기본 정보:

1. 식사 원칙:
- 단백질 위주의 식단 구성
- 탄수화물 조절
- 충분한 수분 섭취

2. 권장 식품:
- 닭가슴살, 생선, 계란
- 채소류
- 견과류 (소량)

3. 식사 시간과 방법:
- 하루 3끼 규칙적인 식사
- 과식 피하기
- 천천히 씹어서 먹기

4. 주의사항:
- 야식 피하기
- 폭식 주의
- 과도한 제한 금지
"""

# data 폴더에 저장
with open("data/diet_info.txt", "w", encoding="utf-8") as f:
   f.write(diet_info)

# GPU 사용 가능 여부 확인
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 모델과 토크나이저 초기화
model_id = "MBZUAI/LaMini-Flan-T5-248M"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(
   model_id,
   torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
   device_map="auto"
)

# Text generation pipeline 생성
pipe = pipeline(
   "text2text-generation",
   model=model,
   tokenizer=tokenizer,
   max_length=512,
   do_sample=True,
   temperature=0.7,
   top_p=0.95,
   repetition_penalty=1.15
)

# Langchain용 LLM 생성
llm = HuggingFacePipeline(pipeline=pipe)

# 한국어 임베딩 모델 초기화
embeddings = HuggingFaceEmbeddings(
   model_name="jhgan/ko-sroberta-multitask",
   model_kwargs={'device': device}
)

# 텍스트 분할기 설정
text_splitter = RecursiveCharacterTextSplitter(
   chunk_size=500,
   chunk_overlap=50,
   length_function=len,
   separators=["\n\n", "\n", ".", "!", "?", ",", " "]
)

# 데이터 로드 및 처리
loader = DirectoryLoader("data", glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()
texts = text_splitter.split_documents(documents)

# 벡터 DB 생성
vectorstore = Chroma.from_documents(
   documents=texts,
   embedding=embeddings,
   persist_directory="./vectorstore"
)

# 메모리 설정
memory = ConversationBufferMemory(
   memory_key="chat_history",
   output_key="answer",
   return_messages=True
)

# 프롬프트 템플릿 설정
template = """다음 내용을 바탕으로 상세하게 한국어로 답변해주세요.

질문: {question}

참고 정보:
{context}

답변은 다음과 같은 형식으로 작성해주세요:
1. 질문에 대한 직접적인 답변
2. 관련된 추가 조언이나 설명
3. 주의사항이나 권장사항

답변:"""

PROMPT = PromptTemplate(
   template=template,
   input_variables=["context", "question"]
)

# QA 체인 생성
qa_chain = ConversationalRetrievalChain.from_llm(
   llm=llm,
   retriever=vectorstore.as_retriever(search_kwargs={"k": 1}),
   memory=memory,
   return_source_documents=True,
   verbose=True,
   combine_docs_chain_kwargs={'prompt': PROMPT}
)

# 대화형 인터페이스
def chat():
   print("AI 챗봇이 시작되었습니다. 종료하려면 'quit' 또는 'exit'를 입력하세요.")
   print("-" * 50)

   while True:
       user_input = input("\n사용자: ")

       if user_input.lower() in ['quit', 'exit']:
           print("\nAI 챗봇을 종료합니다.")
           break

       try:
           # 기본 응답 처리
           if user_input.strip().lower() == "안녕":
               print("\nAI: 안녕하세요! 건강과 관련된 질문이 있으시다면 답변해드리도록 하겠습니다.")
               continue

           # 일반 질문 처리
           response = qa_chain.invoke({
               "question": user_input
           })

           # 응답이 비어있는 경우 처리
           answer = response['answer'].strip()
           if not answer:
               answer = """죄송합니다. 질문하신 내용에 대해 정확한 답변을 드리기 어렵습니다.
               더 구체적으로 질문해 주시거나, 다른 질문을 해주시면 감사하겠습니다."""

           print("\nAI: ", answer)
           print("\n참고 문서:")
           for i, doc in enumerate(response['source_documents'], 1):
               print(f"\n문서 {i}: {doc.page_content}")
           print("-" * 50)

       except Exception as e:
           print(f"\n오류가 발생했습니다: {str(e)}")
           print("다시 질문해 주시면 감사하겠습니다.")

# 챗봇 실행
chat()